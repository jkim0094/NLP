{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnWO37v6oLFg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# !pip install torch==2.2.2 torchtext==0.17.2 torchvision torchaudio --force-reinstall --no-cache-dir\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import spacy\n",
        "import ast\n",
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "INPUT_DIM = len(ingredient_vocab)\n",
        "OUTPUT_DIM = len(recipe_vocab)\n",
        "EMBED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 64\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=recipe_vocab['<pad>'])\n",
        "TRAIN_RATIO = 1.0\n",
        "EPOCHS = 10\n",
        "new_model_train = True\n",
        "model_type = \"Seq2Seq_epoch\"\n",
        "save_model_path = f\"/content/drive/MyDrive/FIT5127-NLP/Ass2/results/{model_type}.pt\"\n",
        "save_history_path = f\"/content/drive/MyDrive/FIT5127-NLP/Ass2/results/{model_type}_history.pt\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ep0r_iLmlIUC",
        "outputId": "2d1a7657-b2c1-4f4d-c165-4ed6cbe97ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.2+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "iuol8CxrHQZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data ë‹¤ìš´\n",
        "train_path = '/content/drive/MyDrive/FIT5127-NLP/Ass2/Cooking_Dataset/train.csv'\n",
        "dev_path = '/content/drive/MyDrive/FIT5127-NLP/Ass2/Cooking_Dataset/dev.csv'\n",
        "test_path = '/content/drive/MyDrive/FIT5127-NLP/Ass2/Cooking_Dataset/test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# ë°ì´í„° í™•ì¸\n",
        "print(f\"Train data size: {len(train_df)}\")\n",
        "print(f\"Dev data size: {len(dev_df)}\")\n",
        "print(f\"Test data size: {len(test_df)}\")\n",
        "print(\"\\nTrain data sample:\")\n",
        "print(train_df.head())\n",
        "#No-Bake Nut Cookies\n",
        "# [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\n",
        "# [\"In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\", \"Stir over medium heat until mixture bubbles all over top.\", \"Boil and stir 5 minutes more. Take off heat.\", \"Stir in vanilla and cereal; mix well.\", \"Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\", \"Let stand until firm, about 30 minutes.\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzpEEDthSnhi",
        "outputId": "3677fb2b-d313-4960-fa2a-f5932daa0af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 162899\n",
            "Dev data size: 1065\n",
            "Test data size: 1081\n",
            "\n",
            "Train data sample:\n",
            "                      Title  \\\n",
            "0       No-Bake Nut Cookies   \n",
            "1               Creamy Corn   \n",
            "2      Reeses Cups(Candy)     \n",
            "3  Cheeseburger Potato Soup   \n",
            "4       Rhubarb Coffee Cake   \n",
            "\n",
            "                                         Ingredients  \\\n",
            "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
            "1  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
            "2  [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
            "3  [\"6 baking potatoes\", \"1 lb. of extra lean gro...   \n",
            "4  [\"1 1/2 c. sugar\", \"1/2 c. butter\", \"1 egg\", \"...   \n",
            "\n",
            "                                              Recipe  \n",
            "0  [\"In a heavy 2-quart saucepan, mix brown sugar...  \n",
            "1  [\"In a slow cooker, combine all ingredients. C...  \n",
            "2  [\"Combine first four ingredients and press in ...  \n",
            "3  [\"Wash potatoes; prick several times with a fo...  \n",
            "4  [\"Cream sugar and butter.\", \"Add egg and beat ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(text, remove_stopwords=True, lemmatize=True):\n",
        "    text_list = ast.literal_eval(text)\n",
        "    tokens = []\n",
        "\n",
        "    with spacy_en.select_pipes(disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())\n",
        "\n",
        "            for token in doc:\n",
        "                # ìˆ«ì ì œê±°\n",
        "                if token.is_digit:\n",
        "                    continue\n",
        "\n",
        "                # ë‹¨ìœ„ ì œê±° (c., tbsp., tsp., oz., lb.)\n",
        "                if any(unit in token.text for unit in ['c.', 'tbsp.', 'tsp.', 'oz.', 'lb.']):\n",
        "                    continue\n",
        "\n",
        "                # ë¶ˆìš©ì–´ ì œê±° (ì„ íƒ)\n",
        "                if remove_stopwords and token.is_stop:\n",
        "                    continue\n",
        "\n",
        "                # ë ˆë§ˆíƒ€ì´ì§• ë˜ëŠ” ì›í˜• ìœ ì§€\n",
        "                if lemmatize:\n",
        "                    tokens.append(token.lemma_)\n",
        "                else:\n",
        "                    tokens.append(token.text)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtQwUvtETjCF",
        "outputId": "86b57a8d-0dd5-4fa7-abdf-0d96cb367749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\n",
            "['1', 'c.', 'firmly', 'packed', 'brown', 'sugar', '1/2', 'c.', 'evaporated', 'milk', '1/2', 'tsp', '.', 'vanilla', '1/2', 'c.', 'broken', 'nuts', '(', 'pecans', ')', '2', 'tbsp', '.', 'butter', 'margarine', '3', '1/2', 'c.', 'bite', 'size', 'shredded', 'rice', 'biscuits']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(token_lists, min_freq=2):\n",
        "    def yield_tokens():\n",
        "        for tokens in token_lists:\n",
        "            yield tokens\n",
        "\n",
        "    vocab = build_vocab_from_iterator(yield_tokens(), min_freq=min_freq,\n",
        "                                      specials=['<pad>', '<sos>', '<eos>', '<unk>'])\n",
        "    vocab.set_default_index(vocab['<unk>'])  # unknown token ì²˜ë¦¬\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "O5IrjwMXuv3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìºì‹œëœ íŒŒì¼ëª…\n",
        "ingredient_cache_path = \"/content/drive/MyDrive/FIT5127-NLP/Ass2/ingredient_tokens.pkl\"\n",
        "recipe_cache_path = \"/content/drive/MyDrive/FIT5127-NLP/Ass2/recipe_tokens.pkl\"\n",
        "\n",
        "# ìºì‹œê°€ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê¸°, ì—†ìœ¼ë©´ í† í°í™”í•´ì„œ ì €ì¥\n",
        "try:\n",
        "    with open(ingredient_cache_path, \"rb\") as f:\n",
        "        ingredient_token_lists = pickle.load(f)\n",
        "    with open(recipe_cache_path, \"rb\") as f:\n",
        "        recipe_token_lists = pickle.load(f)\n",
        "    print(\"Successfully loaded the cached token list!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No cache found â†’ Starting tokenization...\")\n",
        "    ingredient_token_lists = [tokenizer(text) for text in tqdm(train_df['Ingredient'], desc=\"Tokenizing ingredients\")]\n",
        "    recipe_token_lists = [tokenizer(text) for text in tqdm(train_df['Recipe'], desc=\"Tokenizing recipes\")]\n",
        "    # ì €ì¥\n",
        "    with open(ingredient_cache_path, \"wb\") as f:\n",
        "        pickle.dump(ingredient_token_lists, f)\n",
        "    with open(recipe_cache_path, \"wb\") as f:\n",
        "        pickle.dump(recipe_token_lists, f)\n",
        "    print(\"Saved token list!\")\n",
        "\n",
        "\n",
        "\n",
        "# Vocab ìƒì„± (2ë²ˆ ì´ìƒ ë“±ì¥í•œ ë‹¨ì–´ë§Œ í¬í•¨)\n",
        "ingredient_vocab = build_vocab(ingredient_token_lists)\n",
        "recipe_vocab = build_vocab(recipe_token_lists)\n",
        "\n",
        "# í™•ì¸\n",
        "print(\"Ingredient vocab size:\", len(ingredient_vocab))\n",
        "print(\"Recipe vocab size:\", len(recipe_vocab))\n",
        "\n",
        "# ì˜ˆì‹œë¡œ ì¼ë¶€ ë‹¨ì–´ ì¶œë ¥\n",
        "print(\"Sample ingredient tokens:\", list(enumerate(ingredient_vocab.get_itos()))[:10])\n",
        "print(\"Sample recipe tokens:\", list(enumerate(recipe_vocab.get_itos()))[:10])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5_eJ-j6vHST",
        "outputId": "b93bf515-a2ff-48fd-8f46-1b3e32a0bba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ìºì‹œëœ í† í° ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\n",
            "Ingredient vocab size: 5756\n",
            "Recipe vocab size: 8364\n",
            "Sample ingredient tokens: [(0, '<pad>'), (1, '<sos>'), (2, '<eos>'), (3, '<unk>'), (4, '1'), (5, '.'), (6, 'c.'), (7, '2'), (8, '1/2'), (9, ',')]\n",
            "Sample recipe tokens: [(0, '<pad>'), (1, '<sos>'), (2, '<eos>'), (3, '<unk>'), (4, '.'), (5, ','), (6, 'add'), (7, 'mix'), (8, 'minutes'), (9, ';')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,df,ingredient_vocab,recipe_vocab):\n",
        "        self.df = df\n",
        "        self.ingredient_vocab = ingredient_vocab\n",
        "        self.recipe_vocab = recipe_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ í† í°í™”\n",
        "        ingredient_tokens = tokenizer(self.df.iloc[idx][\"Ingredients\"])\n",
        "        recipe_tokens = tokenizer(self.df.iloc[idx][\"Recipe\"])\n",
        "\n",
        "        # ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
        "        ingredient_ids = [self.ingredient_vocab[token] for token in ingredient_tokens]\n",
        "        recipe_ids = [self.recipe_vocab['<sos>']] + [self.recipe_vocab[token] for token in recipe_tokens ]+[self.recipe_vocab['<eos>']]\n",
        "\n",
        "        # í…ì„œë¡œ\n",
        "        return torch.tensor(ingredient_ids), torch.tensor(recipe_ids)"
      ],
      "metadata": {
        "id": "u3aRuH1BcoQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    ingredients, recipes = zip(*batch)\n",
        "    ingredients_padded = pad_sequence(ingredients, batch_first=True, padding_value=ingredient_vocab['<pad>'])\n",
        "    recipes_padded = pad_sequence(recipes, batch_first=True, padding_value=recipe_vocab['<pad>'])\n",
        "    return ingredients_padded.to(DEVICE), recipes_padded.to(DEVICE)\n"
      ],
      "metadata": {
        "id": "zvtpEXhJyzRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_df, ingredient_vocab, recipe_vocab)\n",
        "dev_dataset = CustomDataset(dev_df, ingredient_vocab, recipe_vocab)\n",
        "test_dataset = CustomDataset(test_df, ingredient_vocab, recipe_vocab)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# í™•ì¸\n",
        "sample_ingredients, sample_recipes = next(iter(train_loader))\n",
        "print(\"Ingredients batch shape:\", sample_ingredients.shape)\n",
        "print(\"Recipes batch shape:\", sample_recipes.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXb565ney3rG",
        "outputId": "42d16010-314b-4759-fb21-85b9d9535b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingredients batch shape: torch.Size([4, 74])\n",
            "Recipes batch shape: torch.Size([4, 75])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_GRU(nn.Module):\n",
        "    def __init__(self, ingredient_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "        # ì„ë² ë”©\n",
        "        self.embedding = nn.Embedding(ingredient_vocab_size,embedding_dim)\n",
        "\n",
        "        # GRU ë ˆì´ì–´\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # ë“œë¡­ì•„ì›ƒ\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src : [batch_size, src_len]\n",
        "\n",
        "        # ì„ë² ë”©\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded : [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # gru í†µê³¼\n",
        "        outputs, hidden = self.gru(embedded) # h0ë¥¼ ë”°ë¡œ ì£¼ì§€ ì•Šìœ¼ë©´, ë””í´íŠ¸ë¡œ h0ê°€ 0ë¡œ ì´ˆê¸°í™”ë˜ì„œ ë“¤ì–´ê°\n",
        "        # outputs: [batch_size, src_len, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        return hidden\n",
        "\n",
        "class Decoder_GRU(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.recipe_vocab_size = recipe_vocab_size\n",
        "\n",
        "        # ì„ë² ë”©\n",
        "        self.embedding = nn.Embedding(recipe_vocab_size, embedding_dim)\n",
        "\n",
        "        # GRU ë ˆì´ì–´\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # fc ë ˆì´ì–´\n",
        "        self.fc_out = nn.Linear(hidden_dim, recipe_vocab_size)\n",
        "\n",
        "        # ë“œë¡­ì•„ì›ƒ\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input : [batch_size]\n",
        "        input = input.unsqueeze(1)\n",
        "        # input : [batch_size, ë‹¨ì–´ì˜ ê°œìˆ˜=1]\n",
        "\n",
        "        # ì„ë² ë”©\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded : [batch_size, ë‹¨ì–´ì˜ ê°œìˆ˜=1, hidden_dim]\n",
        "\n",
        "        # GRU í†µê³¼\n",
        "        outputs, hidden = self.gru(embedded,hidden)\n",
        "        # outputs: [batch_size, ë‹¨ì–´ì˜ ê°œìˆ˜=1, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        # fc í†µê³¼\n",
        "        prediction = self.fc_out(outputs.squeeze(1))\n",
        "        # prediction: [batch_size, vocab_size]\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, target=None, teacher_forcing_ratio=0.5, max_len=50):\n",
        "        hidden = self.encoder(src)\n",
        "        batch_size = src.size(0)\n",
        "        vocab_size = self.decoder.recipe_vocab_size\n",
        "\n",
        "        # ğŸŸ¢ ì¶”ë¡  ëª¨ë“œ\n",
        "        if target is None:\n",
        "            outputs = []\n",
        "            input_token = torch.tensor([recipe_vocab['<sos>']] * batch_size).to(self.device)\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                output, hidden = self.decoder(input_token, hidden)\n",
        "                top1 = output.argmax(1)\n",
        "                outputs.append(top1.unsqueeze(1))\n",
        "                input_token = top1\n",
        "\n",
        "            return torch.cat(outputs, dim=1)  # [batch_size, max_len]\n",
        "\n",
        "        # ğŸŸ¢ í•™ìŠµ ëª¨ë“œ\n",
        "        target_len = target.shape[1]\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\n",
        "        input_token = target[:, 0]  # <sos>\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden = self.decoder(input_token, hidden)\n",
        "            outputs[:, t, :] = output\n",
        "            top1 = output.argmax(1)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input_token = target[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHuvOqpGoe5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def compute_bleu(model, dataloader, recipe_vocab, max_len=50):\n",
        "    \"\"\"\n",
        "    Seq2Seq ëª¨ë¸ì˜ BLEU ì ìˆ˜ë¥¼ ê³„ì‚° (1~4-gram ê¸°ì¤€)\n",
        "\n",
        "    Args:\n",
        "        model: í•™ìŠµëœ Seq2Seq ëª¨ë¸\n",
        "        dataloader: ê²€ì¦ ë˜ëŠ” í…ŒìŠ¤íŠ¸ DataLoader\n",
        "        recipe_vocab: recipe vocab ê°ì²´ (ì¸ë±ìŠ¤ â†’ ë‹¨ì–´ ë³€í™˜ìš©)\n",
        "        max_len: ìƒì„±í•  ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´\n",
        "\n",
        "    Returns:\n",
        "        bleu_score (float): 0~100 ì‚¬ì´ì˜ BLEU ì ìˆ˜\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, trg_batch in tqdm(dataloader, desc=\"Evaluating BLEU\"):\n",
        "            src_batch = src_batch.to(DEVICE)\n",
        "            trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "            # ìƒì„±ëœ ì˜ˆì¸¡ ì‹œí€€ìŠ¤: [batch_size, max_len]\n",
        "            generated = model(src_batch, target=None, teacher_forcing_ratio=0.0, max_len=max_len)\n",
        "\n",
        "            for i in range(src_batch.size(0)):\n",
        "                pred_tokens = generated[i].tolist()\n",
        "                trg_tokens = trg_batch[i].tolist()\n",
        "\n",
        "                # <eos> í† í°ì´ ë‚˜ì˜¤ë©´ ê±°ê¸°ì„œ ìë¦„\n",
        "                if recipe_vocab['<eos>'] in pred_tokens:\n",
        "                    pred_tokens = pred_tokens[:pred_tokens.index(recipe_vocab['<eos>'])]\n",
        "                if recipe_vocab['<eos>'] in trg_tokens:\n",
        "                    trg_tokens = trg_tokens[:trg_tokens.index(recipe_vocab['<eos>'])]\n",
        "\n",
        "                preds.append(pred_tokens)\n",
        "                targets.append([trg_tokens])  # corpus_bleuëŠ” list of list í•„ìš”\n",
        "\n",
        "    bleu = corpus_bleu(targets, preds) * 100\n",
        "    print(f\"\\nBLEU Score: {bleu:.2f}\")\n",
        "    return bleu\n",
        "\n",
        "\n",
        "def loss_epoch_seq2seq(model, dataloader, criterion, optimizer=None, teacher_forcing_ratio=0.5):\n",
        "    \"\"\"\n",
        "    í•™ìŠµ ë˜ëŠ” í‰ê°€ ë£¨í”„ì—ì„œ 1 epoch ë™ì•ˆ lossë§Œ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.\n",
        "\n",
        "    Args:\n",
        "        model: Seq2Seq ëª¨ë¸\n",
        "        dataloader: DataLoader\n",
        "        criterion: ì†ì‹¤ í•¨ìˆ˜ (CrossEntropyLoss)\n",
        "        optimizer: ì˜µí‹°ë§ˆì´ì € (Noneì´ë©´ eval ëª¨ë“œë¡œ ë™ì‘)\n",
        "        teacher_forcing_ratio: í•™ìŠµ ì¤‘ Teacher Forcing ë¹„ìœ¨\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê·  loss\n",
        "    \"\"\"\n",
        "    model.train() if optimizer else model.eval()\n",
        "    rloss = 0\n",
        "\n",
        "    for src_batch, trg_batch in tqdm(dataloader, leave=False):\n",
        "        src_batch = src_batch.to(DEVICE)\n",
        "        trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "        output = model(src_batch, trg_batch, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        # CrossEntropyLoss ê³„ì‚°ì„ ìœ„í•œ reshape\n",
        "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
        "        trg = trg_batch[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "\n",
        "        rloss += loss.item() * src_batch.shape[0]\n",
        "\n",
        "    return rloss / len(dataloader.dataset)\n",
        "\n",
        "def Train(model, train_loader, val_loader, criterion, optimizer,\n",
        "          EPOCHS, BATCH_SIZE, TRAIN_RATIO,\n",
        "          save_model_path, save_history_path,\n",
        "          ):\n",
        "    \"\"\"\n",
        "    ì´ì–´ì„œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ start_epochì™€ best_val_lossë¥¼ ì¸ìë¡œ ë°›ìŒ\n",
        "    \"\"\"\n",
        "\n",
        "    loss_history = {\"train\": [], \"val\": []}\n",
        "    best_val_loss = float('inf')\n",
        "    train_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        ep_start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        train_loss = loss_epoch_seq2seq(model, train_loader, criterion, optimizer, teacher_forcing_ratio=0.5)\n",
        "        loss_history[\"train\"].append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss = loss_epoch_seq2seq(model, val_loader, criterion, optimizer=None, teacher_forcing_ratio=0.0)\n",
        "        loss_history[\"val\"].append(val_loss)\n",
        "\n",
        "        ep_elapsed_time = time.time() - ep_start_time\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"train_loss\": train_loss,  # train_loss ì €ì¥\n",
        "            }, save_model_path)\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {ep_elapsed_time:.2f}s\")\n",
        "    train_elapsed_time = time.time() - train_start_time\n",
        "    # Save training history\n",
        "    torch.save({\n",
        "        \"loss_history\": loss_history,\n",
        "        \"EPOCHS\": EPOCHS,\n",
        "        \"BATCH_SIZE\": BATCH_SIZE,\n",
        "        \"TRAIN_RATIO\": TRAIN_RATIO\n",
        "    }, save_history_path)\n",
        "\n",
        "    print(f\"\\nTraining Completed! History saved!| Elapsed Time : {train_elapsed_time}\")\n",
        "    return loss_history\n",
        "\n",
        "\n",
        "def Test(model, test_loader, criterion, recipe_vocab):\n",
        "    \"\"\"\n",
        "    í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ì†ì‹¤ê³¼ BLEU ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.\n",
        "\n",
        "    Args:\n",
        "        model: í•™ìŠµëœ Seq2Seq ëª¨ë¸\n",
        "        test_loader: í…ŒìŠ¤íŠ¸ DataLoader\n",
        "        criterion: ì†ì‹¤ í•¨ìˆ˜\n",
        "        recipe_vocab: vocab ê°ì²´ (BLEU ê³„ì‚°ìš©)\n",
        "\n",
        "    Returns:\n",
        "        test_loss (float), bleu_score (float)\n",
        "    \"\"\"\n",
        "    print(\"\\n Testing on test set...\")\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = loss_epoch_seq2seq(model, test_loader, criterion, optimizer=None, teacher_forcing_ratio=0.0)\n",
        "\n",
        "    bleu_score = compute_bleu(model, test_loader, recipe_vocab)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | BLEU Score: {bleu_score:.2f}\")\n",
        "\n",
        "    return test_loss, bleu_score\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z7NLnAN8Mmem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ìƒì„±\n",
        "encoder = Encoder_GRU(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "decoder = Decoder_GRU(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "if new_model_train:\n",
        "    print(\"New model training!\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    loss_history = Train(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=dev_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        EPOCHS=EPOCHS,\n",
        "        BATCH_SIZE=BATCH_SIZE,\n",
        "        TRAIN_RATIO=TRAIN_RATIO,\n",
        "        save_model_path=save_model_path,\n",
        "        save_history_path=save_history_path,\n",
        "    )\n",
        "\n",
        "    # í”Œë¡¯ ê·¸ë¦¬ê¸°\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(range(1, EPOCHS+1), loss_history[\"train\"], label=\"Train Loss\", color=\"blue\")\n",
        "    plt.plot(range(1, EPOCHS+1), loss_history[\"val\"], label=\"Validation Loss\", color=\"red\", linestyle='--')\n",
        "    plt.xlabel(\"Epoch\", fontsize=18)\n",
        "    plt.ylabel(\"Loss\", fontsize=18)\n",
        "    plt.title(\"Training and Validation Loss\", fontsize=20)\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMJsudpjHp_O",
        "outputId": "e6f98c97-7965-4c15-8363-35e66b1a4453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ ì´ì–´ì„œ í•™ìŠµ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "âœ… Previous Epoch 1 Result:\n",
            "Val Loss: 5.1952\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|â–Š         | 3372/40725 [12:40<1:59:55,  5.19it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = torch.load(save_model_path, map_location=DEVICE)\n",
        "# ëª¨ë¸ ë‹¤ì‹œ ìƒì„± (êµ¬ì¡°ë§Œ ë˜‘ê°™ì´)\n",
        "encoder = Encoder_GRU(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "decoder = Decoder_GRU(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "load_model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "# state_dict ë¡œë“œ\n",
        "load_model.load_state_dict(saved_model[\"model_state_dict\"])"
      ],
      "metadata": {
        "id": "jWLCPGGe_JS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… ì´ì œ Test ê°€ëŠ¥\n",
        "Test(load_model, test_loader, criterion, recipe_vocab)"
      ],
      "metadata": {
        "id": "4hFKnvi1_7RI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnWO37v6oLFg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# !pip install torch==2.2.2 torchtext==0.17.2 torchvision torchaudio --force-reinstall --no-cache-dir\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import spacy\n",
        "import ast\n",
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "INPUT_DIM = len(ingredient_vocab)\n",
        "OUTPUT_DIM = len(recipe_vocab)\n",
        "EMBED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 64\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=recipe_vocab['<pad>'])\n",
        "TRAIN_RATIO = 1.0\n",
        "EPOCHS = 10\n",
        "new_model_train = True\n",
        "model_type = \"Seq2Seq_epoch\"\n",
        "save_model_path = f\"/content/drive/MyDrive/FIT5127-NLP/Ass2/results/{model_type}.pt\"\n",
        "save_history_path = f\"/content/drive/MyDrive/FIT5127-NLP/Ass2/results/{model_type}_history.pt\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ep0r_iLmlIUC",
        "outputId": "2d1a7657-b2c1-4f4d-c165-4ed6cbe97ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.2+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "iuol8CxrHQZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data 다운\n",
        "train_path = '/content/drive/MyDrive/FIT5127-NLP/Ass2/Cooking_Dataset/train.csv'\n",
        "dev_path = '/content/drive/MyDrive/FIT5127-NLP/Ass2/Cooking_Dataset/dev.csv'\n",
        "test_path = '/content/drive/MyDrive/FIT5127-NLP/Ass2/Cooking_Dataset/test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"Train data size: {len(train_df)}\")\n",
        "print(f\"Dev data size: {len(dev_df)}\")\n",
        "print(f\"Test data size: {len(test_df)}\")\n",
        "print(\"\\nTrain data sample:\")\n",
        "print(train_df.head())\n",
        "#No-Bake Nut Cookies\n",
        "# [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\n",
        "# [\"In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\", \"Stir over medium heat until mixture bubbles all over top.\", \"Boil and stir 5 minutes more. Take off heat.\", \"Stir in vanilla and cereal; mix well.\", \"Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\", \"Let stand until firm, about 30 minutes.\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzpEEDthSnhi",
        "outputId": "3677fb2b-d313-4960-fa2a-f5932daa0af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 162899\n",
            "Dev data size: 1065\n",
            "Test data size: 1081\n",
            "\n",
            "Train data sample:\n",
            "                      Title  \\\n",
            "0       No-Bake Nut Cookies   \n",
            "1               Creamy Corn   \n",
            "2      Reeses Cups(Candy)     \n",
            "3  Cheeseburger Potato Soup   \n",
            "4       Rhubarb Coffee Cake   \n",
            "\n",
            "                                         Ingredients  \\\n",
            "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
            "1  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
            "2  [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
            "3  [\"6 baking potatoes\", \"1 lb. of extra lean gro...   \n",
            "4  [\"1 1/2 c. sugar\", \"1/2 c. butter\", \"1 egg\", \"...   \n",
            "\n",
            "                                              Recipe  \n",
            "0  [\"In a heavy 2-quart saucepan, mix brown sugar...  \n",
            "1  [\"In a slow cooker, combine all ingredients. C...  \n",
            "2  [\"Combine first four ingredients and press in ...  \n",
            "3  [\"Wash potatoes; prick several times with a fo...  \n",
            "4  [\"Cream sugar and butter.\", \"Add egg and beat ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(text, remove_stopwords=True, lemmatize=True):\n",
        "    text_list = ast.literal_eval(text)\n",
        "    tokens = []\n",
        "\n",
        "    with spacy_en.select_pipes(disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())\n",
        "\n",
        "            for token in doc:\n",
        "                # 숫자 제거\n",
        "                if token.is_digit:\n",
        "                    continue\n",
        "\n",
        "                # 단위 제거 (c., tbsp., tsp., oz., lb.)\n",
        "                if any(unit in token.text for unit in ['c.', 'tbsp.', 'tsp.', 'oz.', 'lb.']):\n",
        "                    continue\n",
        "\n",
        "                # 불용어 제거 (선택)\n",
        "                if remove_stopwords and token.is_stop:\n",
        "                    continue\n",
        "\n",
        "                # 레마타이징 또는 원형 유지\n",
        "                if lemmatize:\n",
        "                    tokens.append(token.lemma_)\n",
        "                else:\n",
        "                    tokens.append(token.text)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtQwUvtETjCF",
        "outputId": "86b57a8d-0dd5-4fa7-abdf-0d96cb367749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\n",
            "['1', 'c.', 'firmly', 'packed', 'brown', 'sugar', '1/2', 'c.', 'evaporated', 'milk', '1/2', 'tsp', '.', 'vanilla', '1/2', 'c.', 'broken', 'nuts', '(', 'pecans', ')', '2', 'tbsp', '.', 'butter', 'margarine', '3', '1/2', 'c.', 'bite', 'size', 'shredded', 'rice', 'biscuits']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(token_lists, min_freq=2):\n",
        "    def yield_tokens():\n",
        "        for tokens in token_lists:\n",
        "            yield tokens\n",
        "\n",
        "    vocab = build_vocab_from_iterator(yield_tokens(), min_freq=min_freq,\n",
        "                                      specials=['<pad>', '<sos>', '<eos>', '<unk>'])\n",
        "    vocab.set_default_index(vocab['<unk>'])  # unknown token 처리\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "O5IrjwMXuv3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 캐시된 파일명\n",
        "ingredient_cache_path = \"/content/drive/MyDrive/FIT5127-NLP/Ass2/ingredient_tokens.pkl\"\n",
        "recipe_cache_path = \"/content/drive/MyDrive/FIT5127-NLP/Ass2/recipe_tokens.pkl\"\n",
        "\n",
        "# 캐시가 있다면 불러오기, 없으면 토큰화해서 저장\n",
        "try:\n",
        "    with open(ingredient_cache_path, \"rb\") as f:\n",
        "        ingredient_token_lists = pickle.load(f)\n",
        "    with open(recipe_cache_path, \"rb\") as f:\n",
        "        recipe_token_lists = pickle.load(f)\n",
        "    print(\"Successfully loaded the cached token list!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No cache found → Starting tokenization...\")\n",
        "    ingredient_token_lists = [tokenizer(text) for text in tqdm(train_df['Ingredient'], desc=\"Tokenizing ingredients\")]\n",
        "    recipe_token_lists = [tokenizer(text) for text in tqdm(train_df['Recipe'], desc=\"Tokenizing recipes\")]\n",
        "    # 저장\n",
        "    with open(ingredient_cache_path, \"wb\") as f:\n",
        "        pickle.dump(ingredient_token_lists, f)\n",
        "    with open(recipe_cache_path, \"wb\") as f:\n",
        "        pickle.dump(recipe_token_lists, f)\n",
        "    print(\"Saved token list!\")\n",
        "\n",
        "\n",
        "\n",
        "# Vocab 생성 (2번 이상 등장한 단어만 포함)\n",
        "ingredient_vocab = build_vocab(ingredient_token_lists)\n",
        "recipe_vocab = build_vocab(recipe_token_lists)\n",
        "\n",
        "# 확인\n",
        "print(\"Ingredient vocab size:\", len(ingredient_vocab))\n",
        "print(\"Recipe vocab size:\", len(recipe_vocab))\n",
        "\n",
        "# 예시로 일부 단어 출력\n",
        "print(\"Sample ingredient tokens:\", list(enumerate(ingredient_vocab.get_itos()))[:10])\n",
        "print(\"Sample recipe tokens:\", list(enumerate(recipe_vocab.get_itos()))[:10])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5_eJ-j6vHST",
        "outputId": "b93bf515-a2ff-48fd-8f46-1b3e32a0bba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 캐시된 토큰 리스트 불러오기 완료!\n",
            "Ingredient vocab size: 5756\n",
            "Recipe vocab size: 8364\n",
            "Sample ingredient tokens: [(0, '<pad>'), (1, '<sos>'), (2, '<eos>'), (3, '<unk>'), (4, '1'), (5, '.'), (6, 'c.'), (7, '2'), (8, '1/2'), (9, ',')]\n",
            "Sample recipe tokens: [(0, '<pad>'), (1, '<sos>'), (2, '<eos>'), (3, '<unk>'), (4, '.'), (5, ','), (6, 'add'), (7, 'mix'), (8, 'minutes'), (9, ';')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,df,ingredient_vocab,recipe_vocab):\n",
        "        self.df = df\n",
        "        self.ingredient_vocab = ingredient_vocab\n",
        "        self.recipe_vocab = recipe_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 전처리된 텍스트 토큰화\n",
        "        ingredient_tokens = tokenizer(self.df.iloc[idx][\"Ingredients\"])\n",
        "        recipe_tokens = tokenizer(self.df.iloc[idx][\"Recipe\"])\n",
        "\n",
        "        # 인덱스로 변환\n",
        "        ingredient_ids = [self.ingredient_vocab[token] for token in ingredient_tokens]\n",
        "        recipe_ids = [self.recipe_vocab['<sos>']] + [self.recipe_vocab[token] for token in recipe_tokens ]+[self.recipe_vocab['<eos>']]\n",
        "\n",
        "        # 텐서로\n",
        "        return torch.tensor(ingredient_ids), torch.tensor(recipe_ids)"
      ],
      "metadata": {
        "id": "u3aRuH1BcoQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    ingredients, recipes = zip(*batch)\n",
        "    ingredients_padded = pad_sequence(ingredients, batch_first=True, padding_value=ingredient_vocab['<pad>'])\n",
        "    recipes_padded = pad_sequence(recipes, batch_first=True, padding_value=recipe_vocab['<pad>'])\n",
        "    return ingredients_padded.to(DEVICE), recipes_padded.to(DEVICE)\n"
      ],
      "metadata": {
        "id": "zvtpEXhJyzRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_df, ingredient_vocab, recipe_vocab)\n",
        "dev_dataset = CustomDataset(dev_df, ingredient_vocab, recipe_vocab)\n",
        "test_dataset = CustomDataset(test_df, ingredient_vocab, recipe_vocab)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# 확인\n",
        "sample_ingredients, sample_recipes = next(iter(train_loader))\n",
        "print(\"Ingredients batch shape:\", sample_ingredients.shape)\n",
        "print(\"Recipes batch shape:\", sample_recipes.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXb565ney3rG",
        "outputId": "42d16010-314b-4759-fb21-85b9d9535b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingredients batch shape: torch.Size([4, 74])\n",
            "Recipes batch shape: torch.Size([4, 75])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_GRU(nn.Module):\n",
        "    def __init__(self, ingredient_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "        # 임베딩\n",
        "        self.embedding = nn.Embedding(ingredient_vocab_size,embedding_dim)\n",
        "\n",
        "        # GRU 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src : [batch_size, src_len]\n",
        "\n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded : [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # gru 통과\n",
        "        outputs, hidden = self.gru(embedded) # h0를 따로 주지 않으면, 디폴트로 h0가 0로 초기화되서 들어감\n",
        "        # outputs: [batch_size, src_len, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        return hidden\n",
        "\n",
        "class Decoder_GRU(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.recipe_vocab_size = recipe_vocab_size\n",
        "\n",
        "        # 임베딩\n",
        "        self.embedding = nn.Embedding(recipe_vocab_size, embedding_dim)\n",
        "\n",
        "        # GRU 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # fc 레이어\n",
        "        self.fc_out = nn.Linear(hidden_dim, recipe_vocab_size)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input : [batch_size]\n",
        "        input = input.unsqueeze(1)\n",
        "        # input : [batch_size, 단어의 개수=1]\n",
        "\n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded : [batch_size, 단어의 개수=1, hidden_dim]\n",
        "\n",
        "        # GRU 통과\n",
        "        outputs, hidden = self.gru(embedded,hidden)\n",
        "        # outputs: [batch_size, 단어의 개수=1, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        # fc 통과\n",
        "        prediction = self.fc_out(outputs.squeeze(1))\n",
        "        # prediction: [batch_size, vocab_size]\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, target=None, teacher_forcing_ratio=0.5, max_len=50):\n",
        "        hidden = self.encoder(src)\n",
        "        batch_size = src.size(0)\n",
        "        vocab_size = self.decoder.recipe_vocab_size\n",
        "\n",
        "        # 🟢 추론 모드\n",
        "        if target is None:\n",
        "            outputs = []\n",
        "            input_token = torch.tensor([recipe_vocab['<sos>']] * batch_size).to(self.device)\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                output, hidden = self.decoder(input_token, hidden)\n",
        "                top1 = output.argmax(1)\n",
        "                outputs.append(top1.unsqueeze(1))\n",
        "                input_token = top1\n",
        "\n",
        "            return torch.cat(outputs, dim=1)  # [batch_size, max_len]\n",
        "\n",
        "        # 🟢 학습 모드\n",
        "        target_len = target.shape[1]\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\n",
        "        input_token = target[:, 0]  # <sos>\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden = self.decoder(input_token, hidden)\n",
        "            outputs[:, t, :] = output\n",
        "            top1 = output.argmax(1)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input_token = target[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHuvOqpGoe5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def compute_bleu(model, dataloader, recipe_vocab, max_len=50):\n",
        "    \"\"\"\n",
        "    Seq2Seq 모델의 BLEU 점수를 계산 (1~4-gram 기준)\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 Seq2Seq 모델\n",
        "        dataloader: 검증 또는 테스트 DataLoader\n",
        "        recipe_vocab: recipe vocab 객체 (인덱스 → 단어 변환용)\n",
        "        max_len: 생성할 최대 문장 길이\n",
        "\n",
        "    Returns:\n",
        "        bleu_score (float): 0~100 사이의 BLEU 점수\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, trg_batch in tqdm(dataloader, desc=\"Evaluating BLEU\"):\n",
        "            src_batch = src_batch.to(DEVICE)\n",
        "            trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "            # 생성된 예측 시퀀스: [batch_size, max_len]\n",
        "            generated = model(src_batch, target=None, teacher_forcing_ratio=0.0, max_len=max_len)\n",
        "\n",
        "            for i in range(src_batch.size(0)):\n",
        "                pred_tokens = generated[i].tolist()\n",
        "                trg_tokens = trg_batch[i].tolist()\n",
        "\n",
        "                # <eos> 토큰이 나오면 거기서 자름\n",
        "                if recipe_vocab['<eos>'] in pred_tokens:\n",
        "                    pred_tokens = pred_tokens[:pred_tokens.index(recipe_vocab['<eos>'])]\n",
        "                if recipe_vocab['<eos>'] in trg_tokens:\n",
        "                    trg_tokens = trg_tokens[:trg_tokens.index(recipe_vocab['<eos>'])]\n",
        "\n",
        "                preds.append(pred_tokens)\n",
        "                targets.append([trg_tokens])  # corpus_bleu는 list of list 필요\n",
        "\n",
        "    bleu = corpus_bleu(targets, preds) * 100\n",
        "    print(f\"\\nBLEU Score: {bleu:.2f}\")\n",
        "    return bleu\n",
        "\n",
        "\n",
        "def loss_epoch_seq2seq(model, dataloader, criterion, optimizer=None, teacher_forcing_ratio=0.5):\n",
        "    \"\"\"\n",
        "    학습 또는 평가 루프에서 1 epoch 동안 loss만 계산하는 함수.\n",
        "\n",
        "    Args:\n",
        "        model: Seq2Seq 모델\n",
        "        dataloader: DataLoader\n",
        "        criterion: 손실 함수 (CrossEntropyLoss)\n",
        "        optimizer: 옵티마이저 (None이면 eval 모드로 동작)\n",
        "        teacher_forcing_ratio: 학습 중 Teacher Forcing 비율\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: 전체 데이터셋에 대한 평균 loss\n",
        "    \"\"\"\n",
        "    model.train() if optimizer else model.eval()\n",
        "    rloss = 0\n",
        "\n",
        "    for src_batch, trg_batch in tqdm(dataloader, leave=False):\n",
        "        src_batch = src_batch.to(DEVICE)\n",
        "        trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "        output = model(src_batch, trg_batch, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        # CrossEntropyLoss 계산을 위한 reshape\n",
        "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
        "        trg = trg_batch[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "\n",
        "        rloss += loss.item() * src_batch.shape[0]\n",
        "\n",
        "    return rloss / len(dataloader.dataset)\n",
        "\n",
        "def Train(model, train_loader, val_loader, criterion, optimizer,\n",
        "          EPOCHS, BATCH_SIZE, TRAIN_RATIO,\n",
        "          save_model_path, save_history_path,\n",
        "          ):\n",
        "    \"\"\"\n",
        "    이어서 학습할 수 있게 start_epoch와 best_val_loss를 인자로 받음\n",
        "    \"\"\"\n",
        "\n",
        "    loss_history = {\"train\": [], \"val\": []}\n",
        "    best_val_loss = float('inf')\n",
        "    train_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        ep_start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        train_loss = loss_epoch_seq2seq(model, train_loader, criterion, optimizer, teacher_forcing_ratio=0.5)\n",
        "        loss_history[\"train\"].append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss = loss_epoch_seq2seq(model, val_loader, criterion, optimizer=None, teacher_forcing_ratio=0.0)\n",
        "        loss_history[\"val\"].append(val_loss)\n",
        "\n",
        "        ep_elapsed_time = time.time() - ep_start_time\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"train_loss\": train_loss,  # train_loss 저장\n",
        "            }, save_model_path)\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {ep_elapsed_time:.2f}s\")\n",
        "    train_elapsed_time = time.time() - train_start_time\n",
        "    # Save training history\n",
        "    torch.save({\n",
        "        \"loss_history\": loss_history,\n",
        "        \"EPOCHS\": EPOCHS,\n",
        "        \"BATCH_SIZE\": BATCH_SIZE,\n",
        "        \"TRAIN_RATIO\": TRAIN_RATIO\n",
        "    }, save_history_path)\n",
        "\n",
        "    print(f\"\\nTraining Completed! History saved!| Elapsed Time : {train_elapsed_time}\")\n",
        "    return loss_history\n",
        "\n",
        "\n",
        "def Test(model, test_loader, criterion, recipe_vocab):\n",
        "    \"\"\"\n",
        "    테스트 데이터셋에서 손실과 BLEU 점수를 계산하는 함수.\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 Seq2Seq 모델\n",
        "        test_loader: 테스트 DataLoader\n",
        "        criterion: 손실 함수\n",
        "        recipe_vocab: vocab 객체 (BLEU 계산용)\n",
        "\n",
        "    Returns:\n",
        "        test_loss (float), bleu_score (float)\n",
        "    \"\"\"\n",
        "    print(\"\\n Testing on test set...\")\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = loss_epoch_seq2seq(model, test_loader, criterion, optimizer=None, teacher_forcing_ratio=0.0)\n",
        "\n",
        "    bleu_score = compute_bleu(model, test_loader, recipe_vocab)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | BLEU Score: {bleu_score:.2f}\")\n",
        "\n",
        "    return test_loss, bleu_score\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z7NLnAN8Mmem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "encoder = Encoder_GRU(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "decoder = Decoder_GRU(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "if new_model_train:\n",
        "    print(\"New model training!\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    loss_history = Train(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=dev_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        EPOCHS=EPOCHS,\n",
        "        BATCH_SIZE=BATCH_SIZE,\n",
        "        TRAIN_RATIO=TRAIN_RATIO,\n",
        "        save_model_path=save_model_path,\n",
        "        save_history_path=save_history_path,\n",
        "    )\n",
        "\n",
        "    # 플롯 그리기\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(range(1, EPOCHS+1), loss_history[\"train\"], label=\"Train Loss\", color=\"blue\")\n",
        "    plt.plot(range(1, EPOCHS+1), loss_history[\"val\"], label=\"Validation Loss\", color=\"red\", linestyle='--')\n",
        "    plt.xlabel(\"Epoch\", fontsize=18)\n",
        "    plt.ylabel(\"Loss\", fontsize=18)\n",
        "    plt.title(\"Training and Validation Loss\", fontsize=20)\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMJsudpjHp_O",
        "outputId": "e6f98c97-7965-4c15-8363-35e66b1a4453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 이어서 학습 시작합니다!\n",
            "✅ Previous Epoch 1 Result:\n",
            "Val Loss: 5.1952\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 3372/40725 [12:40<1:59:55,  5.19it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = torch.load(save_model_path, map_location=DEVICE)\n",
        "# 모델 다시 생성 (구조만 똑같이)\n",
        "encoder = Encoder_GRU(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "decoder = Decoder_GRU(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "load_model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "# state_dict 로드\n",
        "load_model.load_state_dict(saved_model[\"model_state_dict\"])"
      ],
      "metadata": {
        "id": "jWLCPGGe_JS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 이제 Test 가능\n",
        "Test(load_model, test_loader, criterion, recipe_vocab)"
      ],
      "metadata": {
        "id": "4hFKnvi1_7RI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}